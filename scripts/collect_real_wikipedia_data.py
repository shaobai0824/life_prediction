#!/usr/bin/env python3
"""
çœŸå¯¦Wikipediaè³‡æ–™æ”¶é›†è…³æœ¬

ä¿®å¾©ä¸¦åŸ·è¡ŒçœŸå¯¦çš„Wikipedia APIå‘¼å«ï¼Œæ”¶é›†å…·æœ‰çœŸå¯¦åœ–ç‰‡å’Œå£½å‘½è³‡æ–™çš„è¨“ç·´è³‡æ–™
"""

import sys
import os
import logging
import asyncio
from pathlib import Path
import json
import argparse
from datetime import datetime

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root / "src"))

from ml_pipeline.data_collection.wikipedia_collector import WikipediaCollector

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(project_root / 'logs' / 'real_wikipedia_collection.log', encoding='utf-8')
    ]
)

# ç¢ºä¿stdoutä½¿ç”¨UTF-8
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')

logger = logging.getLogger(__name__)


class RealWikipediaDataManager:
    """çœŸå¯¦Wikipediaè³‡æ–™æ”¶é›†ç®¡ç†å™¨"""

    def __init__(self):
        self.project_root = project_root
        self.output_dir = self.project_root / "data" / "collected" / "wikipedia"
        self.image_cache_dir = self.project_root / "data" / "images" / "cache"
        self.logs_dir = self.project_root / "logs"

        # å‰µå»ºå¿…è¦ç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.image_cache_dir.mkdir(parents=True, exist_ok=True)
        self.logs_dir.mkdir(parents=True, exist_ok=True)

    def get_famous_deceased_persons(self) -> list:
        """ç²å–çŸ¥åå·²æ•…äººç‰©åˆ—è¡¨ï¼ˆå°è¦æ¨¡æ¸¬è©¦ç”¨ï¼‰"""
        return [
            "Albert Einstein",
            "Winston Churchill",
            "Franklin D. Roosevelt",
            "John F. Kennedy",
            "Martin Luther King Jr.",
            "Nelson Mandela",
            "Marilyn Monroe",
            "Elvis Presley",
            "Michael Jackson",
            "Princess Diana",
            "Steve Jobs",
            "Muhammad Ali",
            "Audrey Hepburn",
            "Charlie Chaplin",
            "Pablo Picasso"
        ]

    async def test_wikipedia_api_access(self) -> bool:
        """æ¸¬è©¦Wikipedia APIå­˜å–"""
        try:
            import aiohttp

            logger.info("æ¸¬è©¦Wikipedia APIå­˜å–...")

            headers = {
                'User-Agent': 'LifePredictionBot/1.0 (https://example.com/contact; research@example.com) Python/aiohttp'
            }
            async with aiohttp.ClientSession(headers=headers) as session:
                # æ¸¬è©¦åŸºæœ¬APIå‘¼å«
                params = {
                    'action': 'query',
                    'format': 'json',
                    'titles': 'Albert Einstein',
                    'prop': 'info'
                }

                # ä½¿ç”¨æ¨™æº–çš„Wikipedia APIè€Œä¸æ˜¯REST API
                api_url = 'https://en.wikipedia.org/w/api.php'

                async with session.get(api_url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        logger.info("Wikipedia APIå­˜å–æˆåŠŸ")
                        logger.info(f"æ¸¬è©¦è³‡æ–™: {list(data.get('query', {}).get('pages', {}).keys())}")
                        return True
                    else:
                        logger.error(f"Wikipedia APIå­˜å–å¤±æ•—: {response.status}")
                        return False

        except Exception as e:
            logger.error(f"âŒ Wikipedia APIæ¸¬è©¦å¤±æ•—: {e}")
            return False

    async def collect_small_dataset(self, max_persons: int = 10) -> dict:
        """æ”¶é›†å°è¦æ¨¡çœŸå¯¦è³‡æ–™é›†"""
        try:
            logger.info(f"é–‹å§‹æ”¶é›†å°è¦æ¨¡çœŸå¯¦Wikipediaè³‡æ–™é›† (æœ€å¤š {max_persons} äºº)...")

            # åˆå§‹åŒ–æ”¶é›†å™¨
            collector = WikipediaCollector()

            # ç²å–å·²æ•…åäººåˆ—è¡¨
            famous_persons = self.get_famous_deceased_persons()[:max_persons]
            logger.info(f"ç›®æ¨™æ”¶é›†äººç‰©: {famous_persons}")

            # æ”¶é›†è³‡æ–™
            collected_persons = []

            import aiohttp
            headers = {
                'User-Agent': 'LifePredictionBot/1.0 (https://example.com/contact; research@example.com) Python/aiohttp'
            }
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                headers=headers
            ) as session:
                for person_name in famous_persons:
                    try:
                        logger.info(f"æ­£åœ¨æ”¶é›†: {person_name}")

                        # æ‰‹å‹•èª¿ç”¨æ”¶é›†å™¨çš„å…§éƒ¨æ–¹æ³•
                        person_info = await collector._get_person_info(session, person_name)

                        if person_info and person_info.get('birth_date') and person_info.get('death_date'):
                            logger.info(f"âœ… æˆåŠŸç²å– {person_name} çš„åŸºæœ¬è³‡æ–™")

                            # ç²å–ç…§ç‰‡
                            photos = await collector._get_person_photos(session, person_name)
                            logger.info(f"  ç™¼ç¾ {len(photos)} å¼µæ½›åœ¨ç…§ç‰‡")

                            if photos:
                                # å‰µå»ºäººç‰©è³‡æ–™ç‰©ä»¶
                                from ml_pipeline.data_collection.wikipedia_collector import PersonData
                                person_data = PersonData(
                                    name=person_info['name'],
                                    birth_date=person_info['birth_date'],
                                    death_date=person_info['death_date'],
                                    wikipedia_url=f"https://en.wikipedia.org/wiki/{person_name.replace(' ', '_')}",
                                    photos=photos[:3],  # é™åˆ¶æ¯äºº3å¼µç…§ç‰‡
                                    nationality=person_info.get('nationality'),
                                    occupation=person_info.get('occupation'),
                                    cause_of_death=person_info.get('cause_of_death')
                                )

                                collected_persons.append(person_data)
                                logger.info(f"âœ… {person_name} æ”¶é›†å®Œæˆ")
                            else:
                                logger.warning(f"âš ï¸ {person_name} æ²’æœ‰å¯ç”¨ç…§ç‰‡")
                        else:
                            logger.warning(f"âš ï¸ {person_name} åŸºæœ¬è³‡æ–™ä¸å®Œæ•´")

                    except Exception as e:
                        logger.error(f"âŒ æ”¶é›† {person_name} å¤±æ•—: {e}")
                        continue

                    # é¿å…éå¿«è«‹æ±‚
                    await asyncio.sleep(2)

            logger.info(f"æˆåŠŸæ”¶é›† {len(collected_persons)} å€‹äººç‰©è³‡æ–™")

            # ç”Ÿæˆè¨“ç·´è³‡æ–™é›†
            training_dataset = []
            for person in collected_persons:
                if person.photos:
                    for photo in person.photos:
                        if photo.get('photo_date'):
                            remaining_years = person.remaining_lifespan_at_photo(photo['photo_date'])
                            if remaining_years is not None and remaining_years > 0:
                                training_sample = {
                                    'person_name': person.name,
                                    'photo_url': photo['url'],
                                    'photo_date': photo['photo_date'].isoformat() if photo['photo_date'] else None,
                                    'death_date': person.death_date.isoformat(),
                                    'birth_date': person.birth_date.isoformat(),
                                    'remaining_lifespan_years': remaining_years,
                                    'age_at_photo': person.age_at_photo(photo['photo_date']),
                                    'total_lifespan': person.lifespan_years,
                                    'nationality': person.nationality,
                                    'occupation': person.occupation,
                                    'image_width': photo.get('width'),
                                    'image_height': photo.get('height'),
                                    'wikipedia_url': person.wikipedia_url
                                }
                                training_dataset.append(training_sample)

            # ä¿å­˜çµæœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            # ä¿å­˜äººç‰©è³‡æ–™
            persons_file = self.output_dir / f"real_persons_{timestamp}.json"
            persons_data = [person.to_dict() for person in collected_persons]
            with open(persons_file, 'w', encoding='utf-8') as f:
                json.dump(persons_data, f, indent=2, ensure_ascii=False, default=str)

            # ä¿å­˜è¨“ç·´è³‡æ–™é›†
            dataset_file = self.output_dir / f"real_training_dataset_{timestamp}.json"
            with open(dataset_file, 'w', encoding='utf-8') as f:
                json.dump(training_dataset, f, indent=2, ensure_ascii=False, default=str)

            # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
            stats = {
                'collection_timestamp': timestamp,
                'persons_collected': len(collected_persons),
                'training_samples': len(training_dataset),
                'average_photos_per_person': len(training_dataset) / max(len(collected_persons), 1),
                'persons_with_photos': len([p for p in collected_persons if p.photos]),
                'files_created': {
                    'persons_file': str(persons_file),
                    'dataset_file': str(dataset_file)
                }
            }

            stats_file = self.output_dir / f"real_collection_stats_{timestamp}.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False, default=str)

            logger.info("âœ… çœŸå¯¦è³‡æ–™æ”¶é›†å®Œæˆ!")
            logger.info(f"æ”¶é›†äººç‰©: {stats['persons_collected']}")
            logger.info(f"è¨“ç·´æ¨£æœ¬: {stats['training_samples']}")
            logger.info(f"è³‡æ–™æª”æ¡ˆ: {dataset_file}")

            return {
                'success': True,
                'stats': stats,
                'dataset_file': str(dataset_file),
                'persons_file': str(persons_file)
            }

        except Exception as e:
            logger.error(f"çœŸå¯¦è³‡æ–™æ”¶é›†å¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e)
            }


async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description='çœŸå¯¦Wikipediaè³‡æ–™æ”¶é›†è…³æœ¬')
    parser.add_argument('--max-persons', type=int, default=10,
                       help='æœ€å¤šæ”¶é›†äººæ•¸ï¼ˆé è¨­10äººï¼‰')
    parser.add_argument('--test-api', action='store_true',
                       help='åƒ…æ¸¬è©¦Wikipedia APIå­˜å–')

    args = parser.parse_args()

    print("å£½å‘½é æ¸¬ç³»çµ± - çœŸå¯¦Wikipediaè³‡æ–™æ”¶é›†")
    print("=" * 50)

    try:
        manager = RealWikipediaDataManager()

        if args.test_api:
            # æ¸¬è©¦APIå­˜å–
            success = await manager.test_wikipedia_api_access()
            if success:
                print("âœ… Wikipedia APIæ¸¬è©¦æˆåŠŸ")
                return 0
            else:
                print("âŒ Wikipedia APIæ¸¬è©¦å¤±æ•—")
                return 1
        else:
            # æ”¶é›†çœŸå¯¦è³‡æ–™
            result = await manager.collect_small_dataset(max_persons=args.max_persons)

            print("=" * 50)
            if result['success']:
                print("ğŸ‰ çœŸå¯¦Wikipediaè³‡æ–™æ”¶é›†æˆåŠŸ!")
                stats = result['stats']
                print(f"æ”¶é›†äººç‰©: {stats['persons_collected']}")
                print(f"è¨“ç·´æ¨£æœ¬: {stats['training_samples']}")
                print(f"è³‡æ–™æª”æ¡ˆ: {result['dataset_file']}")
            else:
                print("âŒ çœŸå¯¦Wikipediaè³‡æ–™æ”¶é›†å¤±æ•—!")
                print(f"éŒ¯èª¤: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                return 1

    except KeyboardInterrupt:
        print("\nç”¨æˆ¶ä¸­æ–·åŸ·è¡Œ")
        return 1
    except Exception as e:
        print(f"\nç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))