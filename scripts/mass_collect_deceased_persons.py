#!/usr/bin/env python3
"""
å¤§é‡æ”¶é›†å·²æ•…äººç‰©è³‡æ–™è…³æœ¬

ä½¿ç”¨Wikipediaçš„åˆ†é¡å’Œæœç´¢APIï¼Œè‡ªå‹•ç™¼ç¾ä¸¦æ”¶é›†ç¬¦åˆæ¢ä»¶çš„å·²æ•…äººç‰©è³‡æ–™ï¼š
- ä¸éœ€è¦é å…ˆæŒ‡å®šäººå
- æŒ‰æ­»äº¡å¹´ä»½ç¯„åœæœç´¢
- è‡ªå‹•éæ¿¾æ¢ä»¶ï¼ˆæœ‰ç…§ç‰‡ã€æœ‰ç”Ÿå’å¹´æœˆã€åœ–ç‰‡å“è³ªç­‰ï¼‰
- å¤§é‡ä¸¦è¡Œè™•ç†
"""

import sys
import os
import logging
import asyncio
import aiohttp
from pathlib import Path
import json
import argparse
from datetime import datetime, date
from typing import Dict, List, Any, Set, Optional
import random

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root / "src"))

from ml_pipeline.data_collection.wikipedia_collector import WikipediaCollector
from ml_pipeline.data_collection.image_downloader import RealImageDownloader

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(project_root / 'logs' / 'mass_collection.log', encoding='utf-8')
    ]
)

# ç¢ºä¿stdoutä½¿ç”¨UTF-8
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')

logger = logging.getLogger(__name__)


class MassDeceasedCollector:
    """å¤§é‡å·²æ•…äººç‰©æ”¶é›†å™¨"""

    def __init__(self):
        self.project_root = project_root
        self.output_dir = self.project_root / "data" / "collected" / "mass_search"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.collector = WikipediaCollector()
        self.image_downloader = RealImageDownloader()

        self.wiki_api_url = "https://en.wikipedia.org/w/api.php"

        # çµ±è¨ˆè³‡æ–™
        self.stats = {
            'pages_searched': 0,
            'deceased_found': 0,
            'with_photos': 0,
            'successfully_collected': 0,
            'errors': 0
        }

    def get_death_year_categories(self, start_year: int, end_year: int) -> List[str]:
        """ç”ŸæˆæŒ‰æ­»äº¡å¹´ä»½çš„Wikipediaåˆ†é¡åç¨±"""
        categories = []
        for year in range(start_year, end_year + 1):
            categories.extend([
                f"{year} deaths",
                f"Deaths in {year}",  # æŸäº›åˆ†é¡ä½¿ç”¨é€™ç¨®æ ¼å¼
            ])
        return categories

    def get_additional_death_categories(self) -> List[str]:
        """ç²å–å…¶ä»–æ­»äº¡ç›¸é—œçš„åˆ†é¡"""
        return [
            "20th-century deaths",
            "21st-century deaths",
            "People who died in the 1900s",
            "People who died in the 2000s",
            "Deceased people",
            "Deaths by year",
        ]

    async def search_category_members(
        self,
        session: aiohttp.ClientSession,
        category: str,
        limit: int = 100
    ) -> List[str]:
        """æœç´¢åˆ†é¡ä¸­çš„æˆå“¡"""
        try:
            params = {
                'action': 'query',
                'format': 'json',
                'list': 'categorymembers',
                'cmtitle': f'Category:{category}',
                'cmlimit': limit,
                'cmtype': 'page',
                'cmnamespace': 0  # åªè¦æ–‡ç« å‘½åç©ºé–“
            }

            async with session.get(self.wiki_api_url, params=params) as response:
                if response.status != 200:
                    return []

                data = await response.json()

                if 'query' not in data or 'categorymembers' not in data['query']:
                    return []

                members = []
                for member in data['query']['categorymembers']:
                    title = member.get('title', '').strip()
                    if title and ':' not in title:  # æ’é™¤é‡å®šå‘å’Œå…¶ä»–å‘½åç©ºé–“
                        members.append(title)

                logger.debug(f"åˆ†é¡ '{category}' æ‰¾åˆ° {len(members)} å€‹æˆå“¡")
                return members

        except Exception as e:
            logger.error(f"æœç´¢åˆ†é¡ '{category}' å¤±æ•—: {e}")
            return []

    async def search_by_death_keyword(
        self,
        session: aiohttp.ClientSession,
        search_terms: List[str],
        limit: int = 100
    ) -> List[str]:
        """ä½¿ç”¨é—œéµå­—æœç´¢å·²æ•…äººç‰©"""
        found_titles = set()

        for term in search_terms:
            try:
                params = {
                    'action': 'query',
                    'format': 'json',
                    'list': 'search',
                    'srsearch': term,
                    'srlimit': limit,
                    'srnamespace': 0,
                    'srprop': 'snippet'
                }

                async with session.get(self.wiki_api_url, params=params) as response:
                    if response.status != 200:
                        continue

                    data = await response.json()

                    if 'query' not in data or 'search' not in data['query']:
                        continue

                    for result in data['query']['search']:
                        title = result.get('title', '').strip()
                        snippet = result.get('snippet', '').lower()

                        # æª¢æŸ¥æ˜¯å¦å«æœ‰æ­»äº¡ç›¸é—œè©å½™
                        death_indicators = ['died', 'death', 'deceased', 'passed away', 'd.']
                        if title and any(indicator in snippet for indicator in death_indicators):
                            found_titles.add(title)

                logger.debug(f"é—œéµå­— '{term}' æ‰¾åˆ° {len(found_titles)} å€‹çµæœ")

            except Exception as e:
                logger.error(f"é—œéµå­—æœç´¢ '{term}' å¤±æ•—: {e}")

        return list(found_titles)

    async def discover_deceased_persons(
        self,
        session: aiohttp.ClientSession,
        start_year: int = 2000,
        end_year: int = 2023,
        max_persons: int = 100
    ) -> List[str]:
        """ç™¼ç¾å·²æ•…äººç‰©"""
        logger.info(f"é–‹å§‹æœç´¢ {start_year}-{end_year} å¹´é–“éä¸–çš„äººç‰©...")

        all_candidates = set()

        # 1. æŒ‰å¹´ä»½åˆ†é¡æœç´¢
        year_categories = self.get_death_year_categories(start_year, end_year)

        for category in year_categories:
            members = await self.search_category_members(session, category, limit=50)
            all_candidates.update(members)
            self.stats['pages_searched'] += len(members)

            if len(all_candidates) >= max_persons * 3:  # æ”¶é›†æ›´å¤šå€™é¸è€…ä»¥ä¾¿éæ¿¾
                break

            await asyncio.sleep(1)  # é¿å…éå¿«è«‹æ±‚

        # 2. ä½¿ç”¨é—œéµå­—æœç´¢è£œå……
        if len(all_candidates) < max_persons * 2:
            search_terms = [
                f"died {year}" for year in range(start_year, end_year + 1, 5)
            ] + [
                "biography death",
                "obituary",
                "died aged",
                "passed away"
            ]

            keyword_results = await self.search_by_death_keyword(session, search_terms[:5], limit=30)
            all_candidates.update(keyword_results)

        logger.info(f"ç™¼ç¾ {len(all_candidates)} å€‹å€™é¸äººç‰©")

        # éš¨æ©ŸåŒ–é †åºï¼Œé¿å…ç¸½æ˜¯æ”¶é›†ç›¸åŒçš„äººç‰©
        candidates_list = list(all_candidates)
        random.shuffle(candidates_list)

        return candidates_list[:max_persons * 2]  # è¿”å›å…©å€æ•¸é‡ä»¥ä¾¿å¾ŒçºŒéæ¿¾

    async def is_valid_deceased_person(
        self,
        session: aiohttp.ClientSession,
        title: str
    ) -> Dict[str, Any]:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„å·²æ•…äººç‰©"""
        try:
            person_info = await self.collector._get_person_info(session, title)

            if not person_info:
                return {'valid': False, 'reason': 'ç„¡æ³•ç²å–åŸºæœ¬è³‡è¨Š'}

            # å¿…é ˆæœ‰ç”Ÿå’æ—¥æœŸ
            if not person_info.get('birth_date') or not person_info.get('death_date'):
                return {'valid': False, 'reason': 'ç¼ºå°‘ç”Ÿå’æ—¥æœŸ'}

            # æª¢æŸ¥æ˜¯å¦æœ‰ç…§ç‰‡
            photos = await self.collector._get_person_photos(session, title)
            if len(photos) < 1:
                return {'valid': False, 'reason': 'æ²’æœ‰ç…§ç‰‡'}

            # è¨ˆç®—å¹´é½¡æ˜¯å¦åˆç†
            birth_date = person_info['birth_date']
            death_date = person_info['death_date']

            if not isinstance(birth_date, date) or not isinstance(death_date, date):
                return {'valid': False, 'reason': 'æ—¥æœŸæ ¼å¼ä¸æ­£ç¢º'}

            age = (death_date - birth_date).days // 365
            if age < 10 or age > 120:
                return {'valid': False, 'reason': f'å¹´é½¡ä¸åˆç†: {age}æ­²'}

            return {
                'valid': True,
                'person_info': person_info,
                'photos': photos,
                'age': age
            }

        except Exception as e:
            return {'valid': False, 'reason': f'é©—è­‰å¤±æ•—: {str(e)}'}

    async def collect_batch_persons(
        self,
        session: aiohttp.ClientSession,
        candidates: List[str],
        max_valid: int = 50
    ) -> List[Dict[str, Any]]:
        """æ‰¹æ¬¡æ”¶é›†æœ‰æ•ˆäººç‰©"""
        logger.info(f"é–‹å§‹æ‰¹æ¬¡é©—è­‰ {len(candidates)} å€‹å€™é¸äººç‰©...")

        valid_persons = []
        processed = 0

        for title in candidates:
            if len(valid_persons) >= max_valid:
                break

            try:
                processed += 1
                logger.info(f"[{processed}/{len(candidates)}] é©—è­‰: {title}")

                validation = await self.is_valid_deceased_person(session, title)

                if validation['valid']:
                    logger.info(f"âœ… {title} - æœ‰æ•ˆ (å¹´é½¡: {validation['age']}æ­², ç…§ç‰‡: {len(validation['photos'])}å¼µ)")

                    # ç”Ÿæˆè¨“ç·´è³‡æ–™æ ¼å¼
                    person_data = {
                        'person_name': title,
                        'birth_date': validation['person_info']['birth_date'].isoformat(),
                        'death_date': validation['person_info']['death_date'].isoformat(),
                        'age_at_death': validation['age'],
                        'nationality': validation['person_info'].get('nationality'),
                        'occupation': validation['person_info'].get('occupation'),
                        'photos': validation['photos']
                    }

                    valid_persons.append(person_data)
                    self.stats['successfully_collected'] += 1

                else:
                    logger.debug(f"âŒ {title} - {validation['reason']}")

                self.stats['deceased_found'] += 1

                # é¿å…éå¿«è«‹æ±‚
                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"è™•ç† {title} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                self.stats['errors'] += 1

        logger.info(f"æ‰¹æ¬¡é©—è­‰å®Œæˆï¼Œç²å¾— {len(valid_persons)} å€‹æœ‰æ•ˆäººç‰©")
        return valid_persons

    async def run_mass_collection(
        self,
        start_year: int = 2000,
        end_year: int = 2023,
        max_persons: int = 50,
        output_prefix: str = "mass_collected"
    ) -> Dict[str, Any]:
        """åŸ·è¡Œå¤§é‡æ”¶é›†"""
        try:
            logger.info("é–‹å§‹å¤§é‡å·²æ•…äººç‰©æ”¶é›†...")
            start_time = datetime.now()

            headers = {
                'User-Agent': 'LifePredictionBot/1.0 (https://example.com/contact; research@example.com) Python/aiohttp'
            }

            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                headers=headers
            ) as session:

                # 1. ç™¼ç¾å€™é¸äººç‰©
                candidates = await self.discover_deceased_persons(
                    session, start_year, end_year, max_persons
                )

                if not candidates:
                    return {
                        'success': False,
                        'error': 'æœªæ‰¾åˆ°ä»»ä½•å€™é¸äººç‰©'
                    }

                # 2. æ‰¹æ¬¡æ”¶é›†æœ‰æ•ˆäººç‰©
                valid_persons = await self.collect_batch_persons(
                    session, candidates, max_persons
                )

                # 3. ä¿å­˜çµæœ
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = self.output_dir / f"{output_prefix}_{timestamp}.json"

                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(valid_persons, f, indent=2, ensure_ascii=False, default=str)

                # 4. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
                end_time = datetime.now()
                collection_time = (end_time - start_time).total_seconds()

                stats_report = {
                    'collection_timestamp': timestamp,
                    'search_parameters': {
                        'start_year': start_year,
                        'end_year': end_year,
                        'max_persons': max_persons
                    },
                    'collection_stats': self.stats,
                    'results': {
                        'valid_persons_found': len(valid_persons),
                        'collection_time_seconds': collection_time,
                        'output_file': str(output_file)
                    }
                }

                stats_file = self.output_dir / f"{output_prefix}_stats_{timestamp}.json"
                with open(stats_file, 'w', encoding='utf-8') as f:
                    json.dump(stats_report, f, indent=2, ensure_ascii=False, default=str)

                logger.info("âœ… å¤§é‡æ”¶é›†å®Œæˆ!")
                logger.info(f"æ”¶é›†æ™‚é–“: {collection_time:.1f} ç§’")
                logger.info(f"æœ‰æ•ˆäººç‰©: {len(valid_persons)} å€‹")
                logger.info(f"è³‡æ–™æª”æ¡ˆ: {output_file}")

                return {
                    'success': True,
                    'stats': stats_report,
                    'output_file': str(output_file),
                    'valid_persons': len(valid_persons)
                }

        except Exception as e:
            logger.error(f"å¤§é‡æ”¶é›†å¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e)
            }


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å¤§é‡æ”¶é›†å·²æ•…äººç‰©è³‡æ–™')
    parser.add_argument('--start-year', type=int, default=2010,
                       help='æ­»äº¡å¹´ä»½èµ·å§‹å¹´ (é è¨­: 2010)')
    parser.add_argument('--end-year', type=int, default=2023,
                       help='æ­»äº¡å¹´ä»½çµæŸå¹´ (é è¨­: 2023)')
    parser.add_argument('--max-persons', type=int, default=30,
                       help='æœ€å¤šæ”¶é›†äººæ•¸ (é è¨­: 30)')
    parser.add_argument('--output-prefix', default='mass_collected',
                       help='è¼¸å‡ºæª”æ¡ˆå‰ç¶´ (é è¨­: mass_collected)')

    args = parser.parse_args()

    print("å£½å‘½é æ¸¬ç³»çµ± - å¤§é‡å·²æ•…äººç‰©æ”¶é›†")
    print("=" * 50)
    print(f"æœç´¢ç¯„åœ: {args.start_year}-{args.end_year}")
    print(f"ç›®æ¨™æ”¶é›†: {args.max_persons} äºº")

    async def run():
        collector = MassDeceasedCollector()
        result = await collector.run_mass_collection(
            start_year=args.start_year,
            end_year=args.end_year,
            max_persons=args.max_persons,
            output_prefix=args.output_prefix
        )

        print("=" * 50)
        if result['success']:
            print("ğŸ‰ å¤§é‡æ”¶é›†æˆåŠŸ!")
            print(f"æœ‰æ•ˆäººç‰©: {result['valid_persons']}")
            print(f"è³‡æ–™æª”æ¡ˆ: {result['output_file']}")
            return 0
        else:
            print("âŒ å¤§é‡æ”¶é›†å¤±æ•—!")
            print(f"éŒ¯èª¤: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            return 1

    try:
        return asyncio.run(run())
    except KeyboardInterrupt:
        print("\nç”¨æˆ¶ä¸­æ–·åŸ·è¡Œ")
        return 1
    except Exception as e:
        print(f"\nç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())